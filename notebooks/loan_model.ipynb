{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRUS.AI Loan Model Prep\n",
        "\n",
        "This notebook trains a lightweight loan approval classifier on the Kaggle dataset and exports artifacts for the Node.js backend.\n",
        "\n",
        "## Workflow Outline\n",
        "\n",
        "1. Load raw Kaggle data (CSV files) from `../data/raw/`.\n",
        "2. Perform cleaning, feature engineering, and train/validation split.\n",
        "3. Fit a baseline logistic regression classifier (scikit-learn).\n",
        "4. Compute SHAP values for validation set examples.\n",
        "5. Export model coefficients, encoders, and SHAP summaries to `../data/artifacts/`.\n",
        "6. Generate seed documents for MongoDB collections (`customers`, `loan_applications`).\n",
        "\n",
        "> **Note:** Run this notebook in a Python environment with scikit-learn, pandas, numpy, and shap installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import shap\n",
        "\n",
        "RAW_DATA_DIR = pathlib.Path(\"../data/raw\")\n",
        "ARTIFACT_DIR = pathlib.Path(\"../data/artifacts\")\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Raw data dir exists:\", RAW_DATA_DIR.exists())\n",
        "print(\"Artifacts dir:\", ARTIFACT_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Replace filenames with actual Kaggle CSV paths once downloaded.\n",
        "train_path = RAW_DATA_DIR / \"train_u6lujuX_CVtuZ9i.csv\"\n",
        "\n",
        "df = pd.read_csv(train_path)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_column = \"Loan_Status\"\n",
        "feature_columns = [col for col in df.columns if col not in {target_column, \"Loan_ID\"}]\n",
        "\n",
        "X = df[feature_columns]\n",
        "y = (df[target_column] == \"Y\").astype(int)\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_features = [col for col in feature_columns if col not in numeric_features]\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(classification_report(y_val, model.predict(X_val)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP value computation placeholder.\n",
        "# To keep the notebook light for the hackathon, consider sampling 200 records max.\n",
        "# Example (pseudocode):\n",
        "# explainer = shap.KernelExplainer(model.predict_proba, background_sample)\n",
        "# shap_values = explainer(data_sample)\n",
        "#\n",
        "# TODO:\n",
        "# 1. Derive per-example top feature impacts (positive/negative).\n",
        "# 2. Persist aggregated SHAP outputs to JSON for backend explanations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Artifact export placeholders\n",
        "artifacts = {\n",
        "    \"trained_at\": datetime.utcnow().isoformat(),\n",
        "    \"model_type\": \"logistic_regression\",\n",
        "    \"feature_columns\": feature_columns,\n",
        "    \"numeric_features\": numeric_features,\n",
        "    \"categorical_features\": categorical_features,\n",
        "    # TODO: add coefficient matrix, intercept, encoder metadata\n",
        "}\n",
        "\n",
        "artifacts_path = ARTIFACT_DIR / \"model_metadata.json\"\n",
        "artifacts_path.write_text(pd.Series(artifacts).to_json())\n",
        "print(\"Wrote artifact metadata to\", artifacts_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Generate seed records for MongoDB collections.\n",
        "# Example structure:\n",
        "# seed = {\n",
        "#     \"customers\": [...],\n",
        "#     \"loan_applications\": [...],\n",
        "#     \"shap_explanations\": [...]\n",
        "# }\n",
        "# with open(ARTIFACT_DIR / \"seed_data.json\", \"w\") as fp:\n",
        "#     json.dump(seed, fp, indent=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Actions\n",
        "\n",
        "- [ ] Download Kaggle dataset into `data/raw/`.\n",
        "- [ ] Finalize preprocessing choices and ensure categorical handling matches backend expectations.\n",
        "- [ ] Export trained model coefficients (or pickled model) and serialization helpers.\n",
        "- [ ] Produce sample SHAP explanations and narrative templates.\n",
        "- [ ] Generate MongoDB seed JSON for demo users and loan applications.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
